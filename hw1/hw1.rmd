---
title: HW 1
output: pdf_document
---

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(raster)
library(tidyverse)
library(doMC)
library(foreach)
```

# Problem 1
```{r message=FALSE, warning=FALSE}
N_seq <- seq(100, 2000, 100)
cpu_time <- matrix(NA, nrow = length(N_seq), ncol = 2)
M <- 10; sigma <- 0.2
for (i in seq_along(N_seq)) {
  ## direct use solve
  N <- N_seq[i]
  K <- matrix(rnorm(N*M), nrow=N, ncol=M)
  Sigma <- sigma*diag(rep(1,N))+K%*%t(K)
  t1 <- system.time(solve(Sigma))
  cpu_time[i, 1] <- t1[3]

  ## Sherman-Morrison-Woodbury identity
  ptm <- proc.time()
  I <- diag(rep(1,N))
  C <- sigma * diag(rep(1,M))
  Sigma_inv <- (1/sigma) * (I - K %*% solve(C + t(K)%*%K) %*% t(K))
  t2 <- proc.time() - ptm
  cpu_time[i, 2] <- t2[3]
}
```

## (a)

We first write $\Sigma = \sigma I + KK^T$ using the Sherman-Morrison-Woodbury identity and simplify the equation further.
$$(\sigma I + KK^T)^{-1} = 1/\sigma(I_{N\times N} - K(\sigma I_{M\times M} + K^TK)^{-1}K^T)$$
 We look at the computation time of the two methods in the figure below where we change $N$ from 100 to 2000. Sherman-Morrison-Woodbury identity makes the inverse computation of a $M\times M$ matrix as oppossed to a inverse of $N\times N$ matrix. The figure clearly shows that the computation time has drastically reduced for large $N$.

```{r message=FALSE, warning=FALSE}
plot(N_seq, cpu_time[, 1],
  type = "l", col = "red",
  xlab = "N", ylab = "CPU time (s)")
lines(N_seq, cpu_time[, 2], col = "blue")
legend("topleft", legend = c("direct use solve",
  "Sherman-Morrison-Woodbury identity"), col = c("red", "blue"), lty = 1)
```

## (b)

For the first algorithm the number of FLOPs is $N^3 + N^2(2M-1) + N^2$ and for the second algorithm FLOPs can be computed as below,
- $\sigma I_{M\times M}$ needs $M^2$ FLOPs
- $K^TK$ needs $M^2(2N-1)$ FLOPs
- Sum of the above to needs $M^2$ FLOPs
- Getting inverse of $M\times M $ matrix needs $M^3$ FLOPs
- $(\sigma I_{M\times M} + K^TK)^{-1}K^T$ product needs $MN(2M-1)$ FLOPs
- product of $K$ and above term needs $NM(2M-1)$ FLOPS
- $I$ minus above term needs $N^2$ FLOPs
- Division by $\sigma$ needs another $N^2$ FLOPS.
Summing all the FLOPs gives $2M^2 + M^3 + 2N^2 + M^2(2N-1) + 2MN(2M-1)$
```{r message=FALSE, warning=FALSE}
alg_one <- function(N, M){
  y = N*N*N + N*N*(2*M-1) + N*N
  return(y)
}

alg_two <- function(N, M){
  y = 2*M*M + M*M*M + 2*N*N + M*M*(2*N-1) + 2*M*N*(2*M-1)
  return(y)
}

N <- seq(1, 100, 1)
y1 <- alg_one(N = N, M = 10)
y2 <- alg_two(N = N, M = 10)
plot(N, y1, type ='l', col = 'red', xlab = "N", ylab = "FLOPs")
abline(N, y2, col = "blue")
legend("topleft", legend = c("direct use solve",
  "Sherman-Morrison-Woodbury identity"), col = c("red", "blue"), lty = 1)
```

## (c)

Based on the figure in part 2 we can see that the number of FLOPs used for Sherman-Morrison-Woodbury algorithm is very small compared to the direct solve approach. Hence figure in part one makes sense because the computation time has reduced significantly in retrospect. Them main saving came from the fact that in algorithm our computation is in $O(M^3)$ while in algorithm 1 is in $O(N^2)$ which can make a significant difference when $M$ is much smaller than $N$.

# Problem 2
```{r message=FALSE, warning=FALSE}
GSE1000 <- read.csv("./GSE1000_series_matrix.csv")
gse_data <- GSE1000[, 2:11]
gse_data <- scale(gse_data, center = TRUE, scale = FALSE)
```

## (a)
```{r message=FALSE, warning=FALSE}
gse_svd <- svd(gse_data)
gse_u <- gse_svd$u
gse_d <- gse_svd$d
gse_vt <- t(gse_svd$v)
image(gse_vt, xlab = "Arrays", ylab = "Eigenvalues")
```

## (b)
```{r message=FALSE, warning=FALSE}
p <- gse_d^2 / sum(gse_d^2)
p <- sort(p, decreasing = FALSE)
barplot(p, names.arg = 1:10, horiz = TRUE, xlab = "Eigenexpression Fraction")
```

```{r message=FALSE, warning=FALSE}
entropy <- -1/log(10) * sum(p * log(p))
entropy
```

## (c)
```{r message=FALSE, warning=FALSE}
plot(1:10, gse_vt[1, ],
  col = "red",
  type = "l",
  ylim = c(-1, 1), xlab = "Arrays", ylab = "Eigenexpression Level")
lines(1:10, gse_vt[2, ], col = "blue")
abline(h = 0)
```

## (d)
```{r message=FALSE, warning=FALSE}
plot(raster(gse_u), xlab = "Arrays", ylab = "Genes")
```

# Problem 3
```{r message=FALSE, warning=FALSE}
## Problem 3
N <- 100
n_sample <- seq(20, 1000, N)
df <- data.frame(matrix(ncol = 5, nrow = N * length(n_sample)))
colnames(df) <- c("n", "alpha_MLE", "beta_MLE", "alpha_M", "beta_M")

## Estimation using different methods
idx <- 0
for (i in 1:100) {
  for (n in n_sample) {
    idx <- idx + 1
    df$n[idx] <- n
    direct_est <- rgamma(n, shape = 3, scale = 7)
    ## Estimation MLE method
    mu <- mean(direct_est)
    sigma2 <- var(direct_est)
    s <- log(mu) - mean(log(direct_est))
    df$alpha_MLE[idx] <- (3 - s + sqrt((s-3)*(s-3) + 24*s))/(12 * s)
    df$beta_MLE[idx] <- mu/df$alpha_MLE[idx]
    ## Estimation Moment method
    df$alpha_M[idx] <- mu*mu/sigma2
    df$beta_M[idx] <- sigma2/mu
  }
}

## compute bias
df %>% group_by(n) %>%
  summarise(
    alpha_MLE_bias = mean(alpha_MLE - 3),
    beta_MLE_bias = mean(beta_MLE - 7),
    alpha_M_bias = mean(alpha_M - 3),
    beta_M_bias = mean(beta_M - 7)) %>%
  ggplot(aes(x = n)) +
  geom_line(aes(y = alpha_MLE_bias, color = "alpha_MLE")) +
  geom_line(aes(y = beta_MLE_bias, color = "beta_MLE")) +
  geom_line(aes(y = alpha_M_bias, color = "alpha_M")) +
  geom_line(aes(y = beta_M_bias, color = "beta_M")) +
  labs(title = "Bias of different methods", x = "Sample size", y = "Bias") +
  scale_color_manual(values = c("red", "blue", "green", "purple"))

# compute mse
df %>% group_by(n) %>%
  summarise(
    alpha_MLE_mse = mean((alpha_MLE - 3)^2),
    beta_MLE_mse = mean((beta_MLE - 7)^2),
    alpha_M_mse = mean((alpha_M - 3)^2),
    beta_M_mse = mean((beta_M - 7)^2)) %>%
  ggplot(aes(x = n)) +
  geom_line(aes(y = alpha_MLE_mse, color = "alpha_MLE")) +
  geom_line(aes(y = beta_MLE_mse, color = "beta_MLE")) +
  geom_line(aes(y = alpha_M_mse, color = "alpha_M")) +
  geom_line(aes(y = beta_M_mse, color = "beta_M")) +
  labs(title = "MSE of different methods", x = "Sample size", y = "MSE") +
  scale_color_manual(values = c("red", "blue", "green", "purple"))
```

# Problem 4
```{r message=FALSE, warning=FALSE}
mcmc <- function(N, m, func, num_cores = 4) {
  ## register cores
  registerDoMC(num_cores)
  ## mcmc
  sample_mcmc <- foreach(iter = 1:N, .combine = c) %dopar% {
    R <- matrix(rnorm(m*m, 0, 1), m, m)
    Sigma <- R %*% t(R)
    func(Sigma)
  }
  return(list(sample = sample_mcmc, se = sd(sample_mcmc) / sqrt(N)))
}
```

## (a)
```{r message=FALSE, warning=FALSE}
trace <- function(X) sum(diag(X))

## m = 100
trace_mcmc <- mcmc(1000, 100, trace)
hist(trace_mcmc$sample, xlab = "Trace", main = "Histogram of trace for m = 100")
print(sprintf("Standard error: %.2f, sample size: %d",
  trace_mcmc$se, 1000))

## m = 1000
trace_mcmc <- mcmc(1000, 1000, trace)
hist(trace_mcmc$sample, xlab = "Trace", main = "Histogram of trace for m = 1000")
print(sprintf("Standard error: %.2f, sample size: %d",
  trace_mcmc$se, 1000))
```

## (b)
```{r message=FALSE, warning=FALSE}
max_eignval <- function(X) {
  eigen(X)$values[1]
}

## m = 100
eig_mcmc <- mcmc(1000, 100, max_eignval)
hist(eig_mcmc$sample, xlab = "Max eigenvalue", main = "Histogram of max eigenvalue for m = 100")
print(sprintf("Standard error: %.2f, sample size: %d",
  eig_mcmc$se, 1000))

## m = 1000
eig_mcmc <- mcmc(1000, 1000, trace)
hist(eig_mcmc$sample, xlab = "Max eigenvalue", main = "Histogram of max eigenvalue for m = 1000")
print(sprintf("Standard error: %.2f, sample size: %d",
  eig_mcmc$se, 1000))
```

## (c)
We plot the approximate expected value of largest eigenvalue, versus m, which m is chosen from 1 to 100. The largest m value we used is 100.
```{r message=FALSE, warning=FALSE}
eig_exp <- c()
m_list <- 1:100
for (m in m_list) {
  eig_mcmc <- mcmc(1000, m, max_eignval)
  eig_exp <- c(eig_exp, mean(eig_mcmc$sample))
  cat(m, "\r")
}
plot(m_list, eig_exp, type = "l", xlab = "m", ylab = "expected value of largest eigenvalue")
```

## (d)
$\text{O}(n^3)$, where n is the dimension of the matrix.

# Problem 5

## (a)
The slow code took 104.307 seconds to run on our computers.

## (b)
We first note that $sin(x)^2 - cos(x)^2 = -cos(2x)$, which can be used to calculate "cos_val". Second, we leverage R's vectorization techniques for efficient computation of "cos_vals" and "v_mat". Specifically, when a vector is passed to the cosine function, R computes the cosine values for each element within the vector. Furthermore, when performing an element-wise product between a vector and a matrix, R broadcasts the vector to match the dimensions of the matrix.

```{r message=FALSE, warning=FALSE}
myfunc <- function(v_s, i_v, iter)
{
  v_mat <- matrix(NA, nrow(v_s), ncol(v_s))
  cos_iter <- cos(iter)

  for (i in 1:nrow(v_s))
  {
    for (j in 1:ncol(v_s))
    {
      d_val = round(i_v[i]%%256)
      v_mat[i, j] = v_s[i, j]*(sin(d_val)*sin(d_val)-cos(d_val)*cos(d_val))/cos_iter;
    }
  }
  return(v_mat)
}

myfunc_new <- function(v_s, i_v, iter)
{
  d_vals <- round(i_v %% 256)
  cos_vals <- -cos(2 * d_vals)
  v_mat <- v_s * cos_vals;
  return(v_mat/cos(iter))
}
```

```{r message=FALSE, warning=FALSE}
N1 <- 1e3; N2 <- 2e3; N_tot <- 64
vi_v <- rep(NA, N1)
vd_s <- matrix(NA, N1, N2)

set.seed(123)
for (i in 1:N1){
  vi_v[i] = i + rnorm(1, sd = sqrt(i)*0.01);
  for (j in 1:N2)
    vd_s[i,j] = j + i
}
Res <- rep(NA, N_tot)
Res_new <- rep(NA, N_tot)

# start benchmark
ptm <- proc.time()

# iterative test loop
for (iter in 1:N_tot)
{
  res_mat <- myfunc(vd_s, vi_v, iter)
  Res[iter] <- mean(res_mat)
}

proc.time() - ptm

ptm <- proc.time()

for (iter in 1:N_tot)
{
  res_mat <- myfunc_new(vd_s, vi_v, iter)
  Res_new[iter] <- mean(res_mat)
}

proc.time() - ptm

head(Res)
head(Res_new)

```

We are able to reduce the user running time significantly from 104.307 seconds to 1.023 seconds. Output shows the times for the two methods. We compared the res vector from the two methods and they give the same answer as shown in the output.

## (c)
For parallel computing, we use the "doMC" package to register the requested cores, and use "foreach" package to assign jobs for each worker.
```{r eval=FALSE, message=FALSE, warning=FALSE}
library(doMC)
library(foreach)

## register workers
args = commandArgs(trailingOnly=TRUE)
num_cores <- as.numeric(args[1])
registerDoMC(num_cores)
```

```{r eval=FALSE, message=FALSE, warning=FALSE}
## Use foreach and print time
ptm <- proc.time()
Res <- foreach(i = 1:num_cores, .combine = c) %dopar% {
  res_mat <- myfunc(vd_s, vi_v, iter)
  mean(res_mat)
}
print(proc.time() - ptm)
```

We use the following shell script to repeat this procedure n times, where n is chosen from {1, 4, 8, 16, 32, 64}.

```{shell eval=FALSE, message=FALSE, warning=FALSE}
echo "### Starting at: $(date) ###"
module purge
module load openmpi
module load miniconda3
source activate py

## Run the python script
for n_workers in 1 4 8 16 32 64
do
    echo
    echo "### Starting with $n_workers worker ###"
    Rscript ~/Workspace/stat9250/hw1/q5_remote.r $n_workers
    echo "### Ending with $n_workers worker ###"
    echo
done
conda deactivate
echo "### Ending at: $(date) ###"
```

We plot the running time plot below. We observe that as the number of cores increases from 1 to 4, the running time improves, due to parallelization. After the initial decrease, the running time increases to 1.474 at 16 cores, which might because of the communication costs or job assigning among cores. Lastly, the subsequent decreases might suggest that job assigning among 32 and 64 cores are efficient.
```{r echo=FALSE, message=FALSE, warning=FALSE}
nodes <- c(1, 4, 8, 16, 32, 64)
plot(nodes, c(1.270, 0.668, 1.056, 1.474, 0.968, 0.733),
  type="l", xlab='number of cores', ylab = "running time")
```

# Problem 6
```{python eval=FALSE, python.reticulate = FALSE}
import numpy as np
mat = np.loadtxt('Mat.dat')
np.savetxt("Mat_T.dat", mat.T)

print(np.mean(mat[:, 0]))
print(np.mean(mat[:, 2]))
```
0.04475862857142858, 0.620249032857143

```{r eval=FALSE, message=FALSE, warning=FALSE}
mat_t <- as.matrix(read.table("Mat_T.dat"))
mean(mat_t[1, ])
mean(mat_t[3, ])
```
0.04475863, 0.620249

We can see that both methods give us the same result.
