---
title: HW 1
output: pdf_document
---

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(raster)
library(tidyverse)
library(doMC)
library(foreach)
```

# Problem 1
```{r message=FALSE, warning=FALSE}
N_seq <- seq(100, 2000, 100)
cpu_time <- matrix(NA, nrow = length(N_seq), ncol = 2)
for (i in seq_along(N_seq)) {
  ## direct use solve
  M <- 10; sigma <- 0.2
  N <- N_seq[i]
  K <- matrix(rnorm(N*M), nrow=N, ncol=M)
  Sigma <- sigma*diag(rep(1,N))+K%*%t(K)
  t1 <- system.time(solve(Sigma))
  cpu_time[i, 1] <- t1[3]

  ## Sherman-Morrison-Woodbury identity
  I <- diag(rep(1,N))
  C <- sigma * diag(rep(1,M))
  ptm <- proc.time()
  Sigma_inv <- (1/sigma) * (I - K %*% solve(C + t(K)%*%K) %*% t(K))
  t2 <- proc.time() - ptm
  cpu_time[i, 2] <- t2[3]
}
```

## (a)
```{r message=FALSE, warning=FALSE}
plot(N_seq, cpu_time[, 1],
  type = "l", col = "red",
  xlab = "N", ylab = "CPU time (s)")
lines(N_seq, cpu_time[, 2], col = "blue")
legend("topleft", legend = c("direct use solve",
  "Sherman-Morrison-Woodbury identity"), col = c("red", "blue"), lty = 1)
```

## (b)
```{r message=FALSE, warning=FALSE}
alg_one <- function(N, M){
  y = N*N*N + 2*N*M + N*N
  return(y)
}

alg_two <- function(N, M){
  y = N*N + 2*N*M*M + N*N*M + M*M*M + M*M
  return(y)
}

N <- seq(1, 100, 1)
y1 <- alg_one(N = N, M = 10)
y2 <- alg_two(N = N, M = 10)
plot(N, y1, type ='l', col = 'red', xlab = "N", ylab = "FLOPs")
abline(N, y2, col = "blue")
legend("topleft", legend = c("direct use solve",
  "Sherman-Morrison-Woodbury identity"), col = c("red", "blue"), lty = 1)
```

# Problem 2
```{r message=FALSE, warning=FALSE}
GSE1000 <- read.csv("./GSE1000_series_matrix.csv")
gse_data <- GSE1000[, 2:11]
gse_data <- scale(gse_data, center = TRUE, scale = FALSE)
```

## (a)
```{r message=FALSE, warning=FALSE}
gse_svd <- svd(gse_data)
gse_u <- gse_svd$u
gse_d <- gse_svd$d
gse_vt <- t(gse_svd$v)
image(gse_vt, xlab = "Arrays", ylab = "Eigenvalues")
```

## (b)
```{r message=FALSE, warning=FALSE}
p <- gse_d^2 / sum(gse_d^2)
p <- sort(p, decreasing = FALSE)
barplot(p, names.arg = 1:10, horiz = TRUE, xlab = "Eigenexpression Fraction")
```

```{r message=FALSE, warning=FALSE}
entropy <- -1/log(10) * sum(p * log(p))
entropy
```

## (c)
```{r message=FALSE, warning=FALSE}
plot(1:10, gse_vt[1, ],
  col = "red",
  type = "l",
  ylim = c(-1, 1), xlab = "Arrays", ylab = "Eigenexpression Level")
lines(1:10, gse_vt[2, ], col = "blue")
abline(h = 0)
```

## (d)
```{r message=FALSE, warning=FALSE}
plot(raster(gse_u), xlab = "Arrays", ylab = "Genes")
```

# Problem 3
```{r message=FALSE, warning=FALSE}
## Problem 3
N <- 100
n_sample <- seq(20, 1000, N)
df <- data.frame(matrix(ncol = 5, nrow = N * length(n_sample)))
colnames(df) <- c("n", "alpha_MLE", "beta_MLE", "alpha_M", "beta_M")

## Estimation using different methods
idx <- 0
for (i in 1:100) {
  for (n in n_sample) {
    idx <- idx + 1
    df$n[idx] <- n
    direct_est <- rgamma(n, shape = 3, scale = 7)
    ## Estimation MLE method
    mu <- mean(direct_est)
    sigma2 <- var(direct_est)
    s <- log(mu) - mean(log(direct_est))
    df$alpha_MLE[idx] <- (3 - s + sqrt((s-3)*(s-3) + 24*s))/(12 * s)
    df$beta_MLE[idx] <- mu/df$alpha_MLE[idx]
    ## Estimation Moment method
    df$alpha_M[idx] <- mu*mu/sigma2
    df$beta_M[idx] <- sigma2/mu
  }
}

## compute bias
df %>% group_by(n) %>%
  summarise(
    alpha_MLE_bias = mean(alpha_MLE - 3),
    beta_MLE_bias = mean(beta_MLE - 7),
    alpha_M_bias = mean(alpha_M - 3),
    beta_M_bias = mean(beta_M - 7)) %>%
  ggplot(aes(x = n)) +
  geom_line(aes(y = alpha_MLE_bias, color = "alpha_MLE")) +
  geom_line(aes(y = beta_MLE_bias, color = "beta_MLE")) +
  geom_line(aes(y = alpha_M_bias, color = "alpha_M")) +
  geom_line(aes(y = beta_M_bias, color = "beta_M")) +
  labs(title = "Bias of different methods", x = "Sample size", y = "Bias") +
  scale_color_manual(values = c("red", "blue", "green", "purple"))

# compute mse
df %>% group_by(n) %>%
  summarise(
    alpha_MLE_mse = mean((alpha_MLE - 3)^2),
    beta_MLE_mse = mean((beta_MLE - 7)^2),
    alpha_M_mse = mean((alpha_M - 3)^2),
    beta_M_mse = mean((beta_M - 7)^2)) %>%
  ggplot(aes(x = n)) +
  geom_line(aes(y = alpha_MLE_mse, color = "alpha_MLE")) +
  geom_line(aes(y = beta_MLE_mse, color = "beta_MLE")) +
  geom_line(aes(y = alpha_M_mse, color = "alpha_M")) +
  geom_line(aes(y = beta_M_mse, color = "beta_M")) +
  labs(title = "MSE of different methods", x = "Sample size", y = "MSE") +
  scale_color_manual(values = c("red", "blue", "green", "purple"))
```

# Problem 4
```{r message=FALSE, warning=FALSE}
mcmc <- function(N, m, func, num_cores = 4) {
  ## register cores
  registerDoMC(num_cores)
  ## mcmc
  sample_mcmc <- foreach(iter = 1:N, .combine = c) %dopar% {
    R <- matrix(rnorm(m*m, 0, 1), m, m)
    Sigma <- R %*% t(R)
    func(Sigma)
  }
  return(list(sample = sample_mcmc, se = sd(sample_mcmc) / sqrt(N)))
}
```

## (a)
```{r message=FALSE, warning=FALSE}
trace <- function(X) sum(diag(X))

## m = 100
trace_mcmc <- mcmc(1000, 100, trace)
hist(trace_mcmc$sample)
print(trace_mcmc$se)

## m = 1000
trace_mcmc <- mcmc(1000, 1000, trace)
hist(trace_mcmc$sample)
print(trace_mcmc$se)
```

## (b)
```{r message=FALSE, warning=FALSE}
max_eignval <- function(X) {
  eigen(X)$values[1]
}

## m = 100
eig_mcmc <- mcmc(1000, 100, max_eignval)
hist(eig_mcmc$sample)
print(eig_mcmc$se)

## m = 1000
eig_mcmc <- mcmc(1000, 1000, trace)
hist(eig_mcmc$sample)
print(eig_mcmc$se)
```

## (c)
```{r message=FALSE, warning=FALSE}
eig_exp <- c()
for (m in 1:100) {
  eig_mcmc <- mcmc(1000, m, max_eignval)
  eig_exp <- c(eig_exp, mean(eig_mcmc$sample))
  cat(m, "\r")
}
plot(1:100, eig_exp, type = "l")
```

## (d)
$\text{O}(n^3)$, where n is the dimension of the matrix.

# Problem 5
## Serial optimization
We first note that $sin(x)^2 - cos(x)^2 = -cos(2x)$, which can be used to calculate "cos_val". Second, we leverage R's vectorization techniques for efficient computation of "cos_vals" and "v_mat". Specifically, when a vector is passed to the cosine function, R computes the cosine values for each element within the vector. Furthermore, when performing an element-wise product between a vector and a matrix, R broadcasts the vector to match the dimensions of the matrix.

```{r eval=FALSE, message=FALSE, warning=FALSE}
myfunc <- function(v_s, i_v, iter)
{
  d_vals <- round(i_v %% 256)
  cos_vals <- -cos(2 * d_vals)
  v_mat <- v_s * cos_vals;
  return(v_mat/cos(iter))
}
```

We are able to reduce the user running time from 104.307 seconds to 1.023 seconds.

## Parallel computing
For parallel computing, we use the "doMC" package to register the requested cores, and use "foreach" package to assign jobs for each worker.
```{r eval=FALSE, message=FALSE, warning=FALSE}
library(doMC)
library(foreach)

## register workers
args = commandArgs(trailingOnly=TRUE)
num_cores <- as.numeric(args[1])
registerDoMC(num_cores)
```

```{r eval=FALSE, message=FALSE, warning=FALSE}
## Use foreach and print time
ptm <- proc.time()
Res <- foreach(i = 1:num_cores, .combine = c) %dopar% {
  res_mat <- myfunc(vd_s, vi_v, iter)
  mean(res_mat)
}
print(proc.time() - ptm)
```

We use the following shell script to repeat this procedure n times, where n is chosen from {1, 4, 8, 16, 32, 64}.

```{shell eval=FALSE, message=FALSE, warning=FALSE}
echo "### Starting at: $(date) ###"
module purge
module load openmpi
module load miniconda3
source activate py

## Run the python script
for n_workers in 1 4 8 16 32 64
do
    echo
    echo "### Starting with $n_workers worker ###"
    Rscript ~/Workspace/stat9250/hw1/q5_remote.r $n_workers
    echo "### Ending with $n_workers worker ###"
    echo
done
conda deactivate
echo "### Ending at: $(date) ###"
```

We plot the running time plot below. We observe that as the number of cores increases from 1 to 4, the running time improves, due to parallelization. After the initial decrease, the running time increases to 1.474 at 16 cores, which might because of the communication costs or job assigning among cores. Lastly, the subsequent decreases might suggest that job assigning among 32 and 64 cores are efficient.
```{r echo=FALSE, message=FALSE, warning=FALSE}
plot(1:6, c(1.270, 0.668, 1.056, 1.474, 0.968, 0.733),
  type="l", xlab='number of cores', ylab = "running time", xaxt = "n")
axis(1, at=1:6, labels=c(1, 4, 8, 16, 32, 64))
```

# Problem 6
```{python eval=FALSE, python.reticulate = FALSE}
import numpy as np
mat = np.loadtxt('Mat.dat')
np.savetxt("Mat_T.dat", mat.T)

print(np.mean(mat[:, 0]))
print(np.mean(mat[:, 2]))
```
0.04475862857142858, 0.620249032857143

```{r eval=FALSE, message=FALSE, warning=FALSE}
mat_t <- as.matrix(read.table("Mat_T.dat"))
mean(mat_t[1, ])
mean(mat_t[3, ])
```
0.04475863, 0.620249
